{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "source": [
    "# Load Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/youtube-sentiments/youtube_labeled.csv', usecols=['text', 'emotion'])\n",
    "\n",
    "df"
   ]
  },
  {
   "source": [
    "# Process Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text']\n",
    "y = df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = df['emotion'].unique()\n",
    "N_EMOTIONS = len(EMOTIONS)\n",
    "N_EMOTIONS"
   ]
  },
  {
   "source": [
    "## Categorical Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_map = {\n",
    "    0: 'constructive feedback/idea',\n",
    "    1: 'negative',\n",
    "    2: 'neutral/other', \n",
    "    3: 'positive', \n",
    "    4: 'sadness', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "source": [
    "## Train and Test Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to model in TensorFlow Hub\n",
    "model_hub_path = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "\n",
    "\n",
    "# Build first module layers using TensorFlow Hub model\n",
    "hub_layer = hub.KerasLayer(model_hub_path, input_shape=[], dtype=tf.string, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST = True\n",
    "\n",
    "if BEST:\n",
    "    model = tf.keras.models.Sequential([\n",
    "        hub_layer,\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.8),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(N_EMOTIONS, activation='sigmoid')\n",
    "    ])\n",
    "else:\n",
    "    model = tf.keras.models.Sequential([\n",
    "        hub_layer,\n",
    "        tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding='causal', activation='relu'),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.5)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.5)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(N_EMOTIONS, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Fit Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Cyclical Learning\n",
    "\n",
    "Resource: https://github.com/bckenstler/CLR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr = CyclicLR(base_lr=0.0003, max_lr=0.003,step_size=2000., mode='triangular2')\n",
    "\n",
    "results = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    callbacks=[clr]\n",
    ")"
   ]
  },
  {
   "source": [
    "## Test Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "source": [
    "## Plot Accuracy and Loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_results(model):\n",
    "    acc = model.history['accuracy']\n",
    "    loss = model.history['loss']\n",
    "    val_acc = model.history['val_accuracy']\n",
    "    val_loss = model.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(10, 5), dpi=100)\n",
    "    plt.plot(epochs, loss, label='Training Loss')\n",
    "    plt.plot(epochs, acc, label='Training Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5), dpi=100)\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  },
  {
   "source": [
    "# Label Unlabeled Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/youtube-sentiments/data.csv', usecols=['text'])\n",
    "\n",
    "unlabeled_df"
   ]
  },
  {
   "source": [
    "## Label and update both the model and the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label the dataset using the model previously trained and a threshold\n",
    "def get_labels(df, model):\n",
    "    # Init result array that maps text to emotion\n",
    "    result_text = []\n",
    "    result_emotion = []\n",
    "\n",
    "    for t in df['text']:\n",
    "        # Get classification from model\n",
    "        probability_prediction = model.predict(np.array([t]))[0]\n",
    "        idx = np.argmax(probability_prediction)\n",
    "        if probability_prediction[idx] >= 0.80:\n",
    "            # Reverse categorical\n",
    "            prediction = to_categorical(idx)\n",
    "            prediction = encoder.inverse_transform([len(prediction)-1])\n",
    "            # Add the result to the arrays\n",
    "            result_text.append(t)\n",
    "            result_emotion.append(prediction[0])\n",
    "            # Remove example from dataset\n",
    "            df.drop(df.loc[df['text']==t].index, inplace=True)\n",
    "\n",
    "    return result_text, result_emotion, df\n",
    "\n",
    "\n",
    "# Function to automate the update of the model and the data through labeling\n",
    "def self_learning(labeled_df, unlabeled_df, model):\n",
    "    # Init counter for the iterations\n",
    "    epoch = 0\n",
    "\n",
    "    new_df = labeled_df.copy()\n",
    "\n",
    "    while len(unlabeled_df) > 0:\n",
    "\n",
    "        # Stop after a while\n",
    "        if epoch == 10: break\n",
    "\n",
    "        new_labeled_text, new_labeled_emotion, unlabeled_df = get_labels(unlabeled_df, model)\n",
    "\n",
    "        # Create a dataframe with the new data\n",
    "        model_labeled_data = pd.DataFrame(data={ 'text': new_labeled_text, 'emotion': new_labeled_emotion })\n",
    "\n",
    "        # Concatenate previous data frame with new data frame\n",
    "\n",
    "        new_df = pd.concat([new_df, model_labeled_data])\n",
    "\n",
    "        # Feature and label\n",
    "        x = new_df['text']\n",
    "        y = new_df['emotion']\n",
    "\n",
    "        # Categorical data\n",
    "        y = encoder.transform(y)\n",
    "        y = to_categorical(y)\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            x,\n",
    "            y,\n",
    "            test_size=0.3\n",
    "        )\n",
    "\n",
    "        results = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=10,\n",
    "            validation_split=0.3,\n",
    "            shuffle=True,\n",
    "            batch_size=1,\n",
    "            verbose=0,\n",
    "            callbacks=[clr]\n",
    "        )\n",
    "\n",
    "        print('Epoch: ',epoch)\n",
    "        print('Data added: ', len(new_labeled_text))\n",
    "        print('Data remaining: ', len(unlabeled_df))\n",
    "        print('Updated dataset dimension: ', len(new_df))\n",
    "        print('Model performance:')\n",
    "        model.evaluate(X_test, y_test)\n",
    "        print('')\n",
    "\n",
    "        # Update the iteration counter\n",
    "        epoch += 1\n",
    "\n",
    "    return model, new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model, updated_df = self_learning(labeled_df=df, unlabeled_df=unlabeled_df, model=model)"
   ]
  },
  {
   "source": [
    "## Label the entire dataset at the same time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the entire dataset and create a new dataframe\n",
    "def label_dataset(df, model):\n",
    "    # Init result array that maps text to emotion\n",
    "    result_text = []\n",
    "    result_emotion = []\n",
    "\n",
    "    for t in df['text']:\n",
    "        # Get classification from model\n",
    "        probability_prediction = model.predict(np.array([t]))[0]\n",
    "        idx = np.argmax(probability_prediction)\n",
    "        # Reverse categorical\n",
    "        prediction = to_categorical(idx)\n",
    "        prediction = encoder.inverse_transform([len(prediction)-1])\n",
    "        # Add the result to the arrays\n",
    "        result_text.append(t)\n",
    "        result_emotion.append(prediction[0])\n",
    "\n",
    "    return result_text, result_emotion\n",
    "\n",
    "result_text, result_emotion = label_dataset(unlabeled_df, model)\n",
    "\n",
    "labeled_by_model_df = pd.DataFrame({ 'text': result_text, 'emotion': result_emotion })\n",
    "labeled_by_model_df.head()"
   ]
  },
  {
   "source": [
    "#### Save dataframe labeled by the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_by_model_df.to_csv('/tmp/labeled_by_model_df.csv', index=False)"
   ]
  },
  {
   "source": [
    "# Test Model with New Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Starting model prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'you are great'\n",
    "\n",
    "prediction = np.argmax(model.predict(np.array([sentence])))\n",
    "\n",
    "decode_map[prediction]"
   ]
  },
  {
   "source": [
    "### Updated model test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'you are great'\n",
    "\n",
    "prediction = np.argmax(updated_model.predict(np.array([sentence])))\n",
    "\n",
    "decode_map[prediction]"
   ]
  }
 ]
}